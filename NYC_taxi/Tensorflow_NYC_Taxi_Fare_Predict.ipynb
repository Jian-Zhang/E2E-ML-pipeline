{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 5 NYC Taxi Fare Prediction with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pytorch_env kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frameworks, Tools, Libs used \n",
    "\n",
    "### Tensorflow,  Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "### In this first cell, we'll load the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from dateutil.parser import parse\n",
    "from pytz import timezone\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "\n",
    "output_dir = \".\"\n",
    "OUTDIR = \".\"\n",
    "OUTPUT_RESULT=\"submission.csv\"\n",
    "#Hyper prameters\n",
    "BUCKETS=20\n",
    "HIDDEN_UNITS = \"128 32 4\"\n",
    "SCALE = 10\n",
    "BATCH_SIZE=32\n",
    "ROWS_TO_READ=40000\n",
    "ROWS_TO_SKIP=10\n",
    "LEARNING_RATE=0.04\n",
    "STEPS_TO_PROCESS=40000\n",
    "STEPS=374000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we'll load our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================Data Load=====================\n",
      "2020-04-02 10:14:56.551929\n",
      "2020-04-02 10:16:51.282087\n",
      "2020-04-02 10:16:51.282423\n",
      "Data Load Consuming Time:\n",
      "0:02:01.896109\n",
      "====================End Data Load==============\n"
     ]
    }
   ],
   "source": [
    "time1=datetime.now()\n",
    "print(\"================Data Load=====================\")\n",
    "df = type('', (), {})()\n",
    "print(datetime.now())\n",
    "df.train = pd.read_csv('./input/trainCleanData.csv')\n",
    "print(datetime.now())\n",
    "\n",
    "print(datetime.now())\n",
    "#df.train.head(10)\n",
    "df.train.describe()\n",
    "time2=datetime.now()\n",
    "data_load_time=time2-time1\n",
    "print(\"Data Load Consuming Time:\")\n",
    "print(data_load_time)\n",
    "print(\"====================End Data Load==============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================Test Data Load=====================\n",
      "2020-04-02 10:16:58.456608\n",
      "====================End Data Load==============\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"================Test Data Load=====================\")\n",
    "test_df = type('', (), {})()\n",
    "print(datetime.now())\n",
    "test_df.test = pd.read_csv('./input/trainCleanData.csv')\n",
    "print(\"====================End Data Load==============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the dataÂ¶\n",
    "####  It's a good idea to get to know your data a little bit before you work with it.\n",
    "\n",
    "#### We'll print out a quick summary of a few useful statistics on each column.\n",
    "\n",
    "#### This will include things like mean, standard deviation, max, min, and various quantiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Time of each ride\n",
    "#### The calcuating the hour and week day for millions of rows is costly so we pre-calcualte all possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time5=datetime.now()\n",
    "##calculate times\n",
    "df.train['nyctime'] = df.train.apply(lambda row: row['pickup_datetime'][:14]+'00:00 UTC', axis=1)\n",
    "##df.test['nyctime'] = df.test.apply(lambda row: row['pickup_datetime'][:14]+'00:00 UTC', axis=1)\n",
    "\n",
    "nycTimes = []\n",
    "def findTimes(timeStr, nycDict, field):\n",
    "    if not(timeStr[:14]+'00:00 UTC' in nycDict):\n",
    "        nycTime = {}\n",
    "        nycTime['time'] = parse(timeStr).astimezone(timezone('US/Eastern'))\n",
    "        nycTime['weekday'] = int(nycTime['time'].weekday())\n",
    "        nycTime['hour'] = int(nycTime['time'].hour)\n",
    "        nycTime['hourSince2000'] = int(((nycTime['time'].year-2009)*366+int(nycTime['time'].strftime(\"%j\")))*25+nycTime['time'].hour)\n",
    "        nycTime['nyctime'] = timeStr[:14]+'00:00 UTC'\n",
    "        nycTimes.append(nycTime)\n",
    "    return \n",
    "\n",
    "minDate=parse(df.train['pickup_datetime'].min())\n",
    "maxDate=parse(df.train['pickup_datetime'].max())\n",
    "while (minDate < maxDate):\n",
    "    findTimes(minDate.strftime(\"%Y-%m-%d %H:%M:%S%z\"),nycTimes,'time')\n",
    "    minDate = minDate + timedelta(hours=1)\n",
    "\n",
    "df.times = pd.DataFrame(nycTimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now join the data frames on the hourly time key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56928 entries, 0 to 56927\n",
      "Data columns (total 5 columns):\n",
      "time             56928 non-null datetime64[ns, US/Eastern]\n",
      "weekday          56928 non-null int64\n",
      "hour             56928 non-null int64\n",
      "hourSince2000    56928 non-null int64\n",
      "nyctime          56928 non-null object\n",
      "dtypes: datetime64[ns, US/Eastern](1), int64(3), object(1)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.train=df.train.join(df.times.set_index('nyctime'), on='nyctime')\n",
    "##df.test=df.test.join(df.times.set_index('nyctime'), on='nyctime')\n",
    "df.times.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================End Data Prepare==================\n",
      "Data Process Consuming Time:\n",
      "0:09:19.378515\n",
      "Data Prepare Consuming time:\n",
      "0:11:21.274624\n"
     ]
    }
   ],
   "source": [
    "# Create feature engineering function that will be used in the input and serving input functions\n",
    "def add_engineered(features):\n",
    "    # this is how you can do feature engineering in TensorFlow\n",
    "    lat1 = features['pickup_latitude']\n",
    "    lat2 = features['dropoff_latitude']\n",
    "    lon1 = features['pickup_longitude']\n",
    "    lon2 = features['dropoff_longitude']\n",
    "    latdiff = (lat1 - lat2)\n",
    "    londiff = (lon1 - lon2)\n",
    "    \n",
    "    # set features for distance with sign that indicates direction\n",
    "    features['latdiff'] = latdiff\n",
    "    features['londiff'] = londiff\n",
    "    dist = (latdiff * latdiff + londiff * londiff)**(0.5)\n",
    "    features['euclidean'] = dist\n",
    "    features['cityBlockDist'] = abs(latdiff) + abs(londiff)\n",
    "    return features\n",
    "\n",
    "df.train = add_engineered(df.train)\n",
    "\n",
    "time6=datetime.now()\n",
    "print(\"=================End Data Prepare==================\")\n",
    "data_process_time=time6-time5\n",
    "print(\"Data Process Consuming Time:\")\n",
    "print(data_process_time)\n",
    "\n",
    "data_prepare_time=data_load_time+data_process_time\n",
    "print(\"Data Prepare Consuming time:\")\n",
    "print(data_prepare_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate times\n",
    "test_df.test['nyctime'] = test_df.test.apply(lambda row: row['pickup_datetime'][:14]+'00:00 UTC', axis=1)\n",
    "##df.test['nyctime'] = df.test.apply(lambda row: row['pickup_datetime'][:14]+'00:00 UTC', axis=1)\n",
    "\n",
    "nycTimes = []\n",
    "def findTimes(timeStr, nycDict, field):\n",
    "    if not(timeStr[:14]+'00:00 UTC' in nycDict):\n",
    "        nycTime = {}\n",
    "        nycTime['time'] = parse(timeStr).astimezone(timezone('US/Eastern'))\n",
    "        nycTime['weekday'] = int(nycTime['time'].weekday())\n",
    "        nycTime['hour'] = int(nycTime['time'].hour)\n",
    "        nycTime['hourSince2000'] = int(((nycTime['time'].year-2009)*366+int(nycTime['time'].strftime(\"%j\")))*25+nycTime['time'].hour)\n",
    "        nycTime['nyctime'] = timeStr[:14]+'00:00 UTC'\n",
    "        nycTimes.append(nycTime)\n",
    "    return \n",
    "\n",
    "minDate=parse(test_df.test['pickup_datetime'].min())\n",
    "maxDate=parse(test_df.test['pickup_datetime'].max())\n",
    "while (minDate < maxDate):\n",
    "    findTimes(minDate.strftime(\"%Y-%m-%d %H:%M:%S%z\"),nycTimes,'time')\n",
    "    minDate = minDate + timedelta(hours=1)\n",
    "\n",
    "test_df.times = pd.DataFrame(nycTimes)\n",
    "test_df.test=test_df.test.join(test_df.times.set_index('nyctime'), on='nyctime')\n",
    "# Create feature engineering function that will be used in the input and serving input functions\n",
    "def add_engineered(features):\n",
    "    # this is how you can do feature engineering in TensorFlow\n",
    "    lat1 = features['pickup_latitude']\n",
    "    lat2 = features['dropoff_latitude']\n",
    "    lon1 = features['pickup_longitude']\n",
    "    lon2 = features['dropoff_longitude']\n",
    "    latdiff = (lat1 - lat2)\n",
    "    londiff = (lon1 - lon2)\n",
    "    \n",
    "    # set features for distance with sign that indicates direction\n",
    "    features['latdiff'] = latdiff\n",
    "    features['londiff'] = londiff\n",
    "    dist = (latdiff * latdiff + londiff * londiff)**(0.5)\n",
    "    features['euclidean'] = dist\n",
    "    features['cityBlockDist'] = abs(latdiff) + abs(londiff)\n",
    "    return features\n",
    "\n",
    "test_df.test = add_engineered(test_df.test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the measure used to see how close the data is to actual taxi fares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(labels, predictions):\n",
    "    pred_values = tf.cast(predictions['predictions'],tf.float64)\n",
    "    return {'rmse': tf.metrics.root_mean_squared_error(labels*SCALE, pred_values*SCALE)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an estimator starting from INPUT COLUMNS.\n",
    "####  These include feature transformations and synthetic features.\n",
    "#### The model is a wide-and-deep model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the raw input columns, and will be provided for prediction also\n",
    "INPUT_COLUMNS = [\n",
    "    # Define features\n",
    "    \n",
    "    # Numeric columns\n",
    "    tf.feature_column.numeric_column('weekday'),\n",
    "    tf.feature_column.numeric_column('hour'),\n",
    "    tf.feature_column.numeric_column('pickup_latitude'),\n",
    "    tf.feature_column.numeric_column('pickup_longitude'),\n",
    "    tf.feature_column.numeric_column('dropoff_latitude'),\n",
    "    tf.feature_column.numeric_column('dropoff_longitude'),\n",
    "    tf.feature_column.numeric_column('passenger_count'),\n",
    "    #tf.feature_column.numeric_column('hourSince2000'),\n",
    "    \n",
    "    # Engineered features that are created in the input_fn\n",
    "    tf.feature_column.numeric_column('latdiff'),\n",
    "    tf.feature_column.numeric_column('londiff'),\n",
    "    tf.feature_column.numeric_column('euclidean'),\n",
    "    tf.feature_column.numeric_column('cityBlockDist')\n",
    "]\n",
    "# Build the estimator\n",
    "def build_estimator(model_dir, nbuckets, hidden_units):\n",
    "    \"\"\"\n",
    "     \n",
    "  \"\"\"\n",
    "\n",
    "    # Input columns   hourSince2000,\n",
    "    (dayofweek, hourofday, plat, plon, dlat, dlon, pcount, latdiff, londiff, euclidean,cityBlockDist) = INPUT_COLUMNS\n",
    "\n",
    "    # Bucketize the times \n",
    "    hourbuckets = np.linspace(0.0, 23.0, 24).tolist()\n",
    "    b_hourofday = tf.feature_column.bucketized_column(hourofday, hourbuckets)\n",
    "    weekdaybuckets = np.linspace(0.0, 6.0, 7).tolist()\n",
    "    b_dayofweek = tf.feature_column.bucketized_column(dayofweek, weekdaybuckets)\n",
    "    #since2000buckets = np.linspace(0.0, 599999, 60000).tolist()\n",
    "    #b_hourSince2000 = tf.feature_column.bucketized_column(hourSince2000, since2000buckets)\n",
    "    \n",
    "    # Bucketize the lats & lons\n",
    "    latbuckets = np.linspace(38.0, 42.0, nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(-76.0, -72.0, nbuckets).tolist()\n",
    "    b_plat = tf.feature_column.bucketized_column(plat, latbuckets)\n",
    "    b_dlat = tf.feature_column.bucketized_column(dlat, latbuckets)\n",
    "    b_plon = tf.feature_column.bucketized_column(plon, lonbuckets)\n",
    "    b_dlon = tf.feature_column.bucketized_column(dlon, lonbuckets)\n",
    "   \n",
    "    # Feature cross\n",
    "    ploc = tf.feature_column.crossed_column([b_plat, b_plon], nbuckets * nbuckets)\n",
    "    dloc = tf.feature_column.crossed_column([b_dlat, b_dlon], nbuckets * nbuckets)\n",
    "    pd_pair = tf.feature_column.crossed_column([ploc, dloc], nbuckets ** 4 )\n",
    "    day_hr =  tf.feature_column.crossed_column([b_dayofweek, b_hourofday], 24 * 7)\n",
    "\n",
    "    # Wide columns and deep columns.\n",
    "    wide_columns = [\n",
    "        # Feature crosses\n",
    "        dloc, ploc, pd_pair,\n",
    "        day_hr,\n",
    "\n",
    "        # Sparse columns\n",
    "        b_dayofweek, b_hourofday,\n",
    "        #b_hourSince2000,\n",
    "\n",
    "        # Anything with a linear relationship\n",
    "        pcount \n",
    "    ]\n",
    "\n",
    "    deep_columns = [\n",
    "        # Embedding_column to \"group\" together ...\n",
    "        tf.feature_column.embedding_column(pd_pair, 10),\n",
    "        tf.feature_column.embedding_column(day_hr, 10),\n",
    "        #tf.feature_column.embedding_column(b_hourSince2000, 60000),\n",
    "        # Numeric columns\n",
    "        plat, plon, dlat, dlon,\n",
    "        latdiff, londiff, euclidean,cityBlockDist\n",
    "    ]\n",
    "    \n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "        model_dir = model_dir,\n",
    "        linear_feature_columns = wide_columns,\n",
    "        dnn_feature_columns = deep_columns,\n",
    "        dnn_hidden_units = hidden_units)\n",
    "\n",
    "    # add extra evaluation metric for hyperparameter tuning\n",
    "      \n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, rmse)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a neural network model\n",
    "#### In this exercise, we'll be trying to predicttaxi fares. Ok get all the features into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weekday',\n",
       " 'hour',\n",
       " 'pickup_latitude',\n",
       " 'pickup_longitude',\n",
       " 'dropoff_latitude',\n",
       " 'dropoff_longitude',\n",
       " 'passenger_count',\n",
       " 'latdiff',\n",
       " 'londiff',\n",
       " 'euclidean',\n",
       " 'cityBlockDist']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns={}\n",
    "for i in INPUT_COLUMNS:\n",
    "    feature_columns[i.key]=i\n",
    "list(feature_columns.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaldf=test_df.test\n",
    "eval_input_fn = tf.estimator.inputs.pandas_input_fn(x = evaldf[list(feature_columns.keys())],\n",
    "                                                    y = evaldf[\"fare_amount\"] / SCALE,  # note the scaling\n",
    "                                                    num_epochs = 1000, \n",
    "                                                    batch_size = len(evaldf), \n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take the panda data and use the estimator functions to turn it into processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf=df.train\n",
    "time7=datetime.now()\n",
    "# Split into train and eval and create input functions\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.pandas_input_fn(x = traindf[list(feature_columns.keys())],\n",
    "                                                    y = traindf[\"fare_amount\"] / SCALE,\n",
    "                                                    num_epochs = 1000,\n",
    "                                                    batch_size = BATCH_SIZE,\n",
    "                                                    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********Begin train****************\n",
      "***********End train******************\n",
      "Model Train Consuming Time:\n",
      "0:00:21.231638\n"
     ]
    }
   ],
   "source": [
    "#tf.logging.set_verbosity(tf.logging.INFO)\n",
    "myopt = tf.train.FtrlOptimizer(learning_rate = LEARNING_RATE) # note the learning rate\n",
    "estimator = estimator = build_estimator(OUTDIR, BUCKETS, HIDDEN_UNITS.split(' '))\n",
    "    \n",
    "estimator = tf.contrib.estimator.add_metrics(estimator,rmse)\n",
    "  \n",
    "train_spec=tf.estimator.TrainSpec(\n",
    "                    input_fn = train_input_fn,max_steps = STEPS_TO_PROCESS)\n",
    "eval_spec=tf.estimator.EvalSpec(\n",
    "                    input_fn = eval_input_fn,\n",
    "                    steps = None,\n",
    "                    start_delay_secs = 1, # start evaluating after N seconds\n",
    "                    throttle_secs = 10,  # evaluate every N seconds\n",
    "                    )\n",
    "print(\"***********Begin train****************\")\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "print(\"***********End train******************\")\n",
    "\n",
    "time8=datetime.now()\n",
    "model_train_time=time8-time7\n",
    "print(\"Model Train Consuming Time:\")\n",
    "print(model_train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(x, y):\n",
    "    return np.sqrt(((x - y) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          29.5\n",
      "1          12.5\n",
      "2           6.9\n",
      "3           7.5\n",
      "4           9.1\n",
      "           ... \n",
      "37931777   11.7\n",
      "37931778    8.5\n",
      "37931779    7.6\n",
      "37931780   31.6\n",
      "37931781    8.2\n",
      "Name: fare_amount, Length: 37931782, dtype: float64\n",
      "0           4.5\n",
      "1           7.7\n",
      "2           5.3\n",
      "3           7.5\n",
      "4          16.5\n",
      "           ... \n",
      "37931777    6.1\n",
      "37931778   12.0\n",
      "37931779    4.2\n",
      "37931780   28.9\n",
      "37931781    7.5\n",
      "Name: fare_amount, Length: 37931782, dtype: float64\n",
      "RMSE Value:\n",
      "19.302091589003933\n",
      "Data Prepare Consuming time:\n",
      "0:11:21.274624\n",
      "Model Train Consuming Time:\n",
      "0:00:21.231638\n"
     ]
    }
   ],
   "source": [
    "evalaution_input_fn = tf.estimator.inputs.pandas_input_fn(x = evaldf[list(feature_columns.keys())],\n",
    "                                                    y = None,  \n",
    "                                                    num_epochs = 1, \n",
    "                                                    batch_size = len(evaldf), \n",
    "                                                    shuffle=False)\n",
    "evlaution_y = evaldf[\"fare_amount\"]\n",
    "evlaution_result=estimator.predict(input_fn=evalaution_input_fn)\n",
    "eval_pred_result = pd.DataFrame({'fare_amount':[i['predictions'][0]*SCALE for i in evlaution_result]})\n",
    "eval_pred_result2=eval_pred_result[\"fare_amount\"]\n",
    "\n",
    "print(eval_pred_result2)\n",
    "print(evlaution_y)\n",
    "RMSE_value=RMSE(eval_pred_result2,evlaution_y)\n",
    "print(\"RMSE Value:\")\n",
    "print(RMSE(eval_pred_result2,evlaution_y))\n",
    "print(\"Data Prepare Consuming time:\")\n",
    "print(data_prepare_time)\n",
    "print(\"Model Train Consuming Time:\")\n",
    "print(model_train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-95a4457c4c76>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-95a4457c4c76>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    RMSE Value:\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# first time\n",
    "RMSE Value:\n",
    "19.302091589003933\n",
    "Data Prepare Consuming time:\n",
    "0:09:56.071902\n",
    "Model Train Consuming Time:\n",
    "0:00:14.203152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second time\n",
    "RMSE Value:\n",
    "19.302091589003933\n",
    "Data Prepare Consuming time:\n",
    "0:11:21.274624\n",
    "Model Train Consuming Time:\n",
    "0:00:21.231638"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
