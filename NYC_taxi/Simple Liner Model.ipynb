{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo1  NYC Taxi Fare Prediction \n",
    "\n",
    "\n",
    "### Basic Starter Kernel example  \n",
    "https://www.kaggle.com/dster/nyc-taxi-fare-starter-kernel-simple-linear-model/notebook\n",
    "\n",
    "### Based on: Pandas, numpy, simple linearl mode\n",
    "\n",
    "### use pytorch_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GCP-Coupons-Instructions.rtf', 'train.csv.zip', 'train.csv', 'sample_submission.csv', 'test.csv']\n"
     ]
    }
   ],
   "source": [
    "# Initial Python environment setup...\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # CSV file I/O (e.g. pd.read_csv)\n",
    "import os # reading the input files we have access to\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "print(os.listdir('./input'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training data\n",
    "First let's read in our training data. Kernels do not yet support enough memory to load the whole dataset at once, at least using pd.read_csv. The entire dataset is about 55M rows, so we're skipping a good portion of the data, but it's certainly possible to build a model using all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load Consuming Time:\n",
      "0:02:21.900109\n"
     ]
    }
   ],
   "source": [
    "time1=datetime.now()\n",
    "\n",
    "\n",
    "train_df =  pd.read_csv('./input/train.csv')\n",
    "train_df.dtypes\n",
    "\n",
    "time2=datetime.now()\n",
    "data_load_time=time2-time1\n",
    "print(\"Data Load Consuming Time:\")\n",
    "print(data_load_time)\n",
    "# Given a dataframe, add two new features 'abs_diff_longitude' and\n",
    "# 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n",
    "# the pickup location to the dropoff location.\n",
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "\n",
    "add_travel_vector_features(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Explore and prune outliers\n",
    "\n",
    "First let's see if there are any NaNs in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key                     0\n",
      "fare_amount             0\n",
      "pickup_datetime         0\n",
      "pickup_longitude        0\n",
      "pickup_latitude         0\n",
      "dropoff_longitude     376\n",
      "dropoff_latitude      376\n",
      "passenger_count         0\n",
      "abs_diff_longitude    376\n",
      "abs_diff_latitude     376\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old size: 55423856\n",
      "New size: 55423480\n",
      "Data Processing Consuming Time:\n",
      "0:00:16.825389\n",
      "Data prepare Consuming Time:\n",
      "0:02:38.725498\n"
     ]
    }
   ],
   "source": [
    "time3=datetime.now()\n",
    "\n",
    "print('Old size: %d' % len(train_df))\n",
    "train_df = train_df.dropna(how = 'any', axis = 'rows')\n",
    "print('New size: %d' % len(train_df))\n",
    "\n",
    "#There are further steps for the data preparation, but we simply skiped it here\n",
    "# E.g., some data ponints longitude > 5\n",
    "# E.g., some data points in water -> https://www.kaggle.com/breemen/nyc-taxi-fare-data-exploration\n",
    "\n",
    "#plot = train_df.iloc[:2000].plot.scatter('abs_diff_longitude', 'abs_diff_latitude')\n",
    "#print('Old size: %d' % len(train_df))\n",
    "#train_df = train_df[(train_df.abs_diff_longitude < 5.0) & (train_df.abs_diff_latitude < 5.0)]\n",
    "#print('New size: %d' % len(train_df))\n",
    "\n",
    "time4=datetime.now()\n",
    "data_processing_time=time4-time3\n",
    "print(\"Data Processing Consuming Time:\")\n",
    "print(data_processing_time)\n",
    "data_prepare_time=data_load_time+data_processing_time\n",
    "print(\"Data prepare Consuming Time:\")\n",
    "print(data_prepare_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect most of these values to be very small (likely between 0 and 1) since it should all be differences between GPS coordinates within one city. For reference, one degree of latitude is about 69 miles. However, we can see the dataset has extreme values which do not make sense. Let's remove those values from our training set. Based on the scatterplot, it looks like we can safely exclude values above 5 (though remember the scatterplot is only showing the first 2000 rows...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model\n",
    "Our model will take the form  **X⋅w=y  where  X  is a matrix of input features, and  y  is a column of the target variable, fare_amount, for each row**. The weight column  w  is what we will \"learn\".\n",
    "\n",
    "First let's setup our input matrix  X  and target column  y  from our training set. The matrix  X  should consist of the two GPS coordinate differences, plus a third term of 1 to allow the model to learn a constant bias term. The column  y  should consist of the target fare_amount values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55423480, 3)\n",
      "(55423480,)\n"
     ]
    }
   ],
   "source": [
    "# Construct and return an Nx3 input matrix for our linear model\n",
    "# using the travel vector, plus a 1.0 for a constant bias term.\n",
    "\n",
    "def get_input_matrix(df):\n",
    "    return np.column_stack((df.abs_diff_longitude, df.abs_diff_latitude, np.ones(len(df))))\n",
    "\n",
    "train_X = get_input_matrix(train_df)\n",
    "train_y = np.array(train_df['fare_amount'])\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use numpy's lstsq （LeaST SQuare） library function to find the optimal weight column  w .\n",
    "\n",
    "a=np.linalg.lstsq(x,b),b=a*x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.14423776e-03 4.63805262e-04 1.13431261e+01]\n",
      "Model Train Consuming Time:\n",
      "0:00:04.007546\n"
     ]
    }
   ],
   "source": [
    "time5=datetime.now()\n",
    "\n",
    "# The lstsq function returns several things, and we only care about the actual weight vector w.\n",
    "(w, _, _, _) = np.linalg.lstsq(train_X, train_y, rcond = None)\n",
    "print(w)\n",
    "\n",
    "time6=datetime.now()\n",
    "model_train_time=time6-time5\n",
    "print(\"Model Train Consuming Time:\")\n",
    "print(model_train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.14423776e-03 4.63805262e-04 1.13431261e+01]\n",
      "Model Train OLS Consuming Time:\n",
      "0:00:01.105275\n"
     ]
    }
   ],
   "source": [
    "#   numpy.matmul \n",
    "time7=datetime.now()\n",
    "\n",
    "w_OLS = np.matmul(np.matmul(np.linalg.inv(np.matmul(train_X.T, train_X)), train_X.T), train_y)\n",
    "print(w_OLS)\n",
    "time8=datetime.now()\n",
    "model_train_time_OLS=time8-time7\n",
    "print(\"Model Train OLS Consuming Time:\")\n",
    "print(model_train_time_OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights pass a quick sanity check, since we'd expect the first two values -- the weights for the absolute longitude and latitude differences -- to be positive, as more distance should imply a higher fare, and we'd expect the bias term to loosely represent the cost of a very short ride.\n",
    "\n",
    "Sidenote: we can actually calculate the weight column  w  directly using the Ordinary Least Squares method:  w=(XT⋅X)−1⋅XT⋅y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction on train.csv data set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Value based on train data set:\n",
      "18.009408916465482\n",
      "['input', 'submission.csv', '.ipynb_checkpoints', 'Simple Liner Model.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_evalutation, y_train, y_evalutation = train_test_split(train_X, train_y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "\n",
    "y_evaluation_result = np.matmul(X_evalutation, w).round(decimals = 2)\n",
    "\n",
    "\n",
    "\n",
    "def rmse(x, y):\n",
    "    return np.sqrt(((x - y) ** 2).mean())\n",
    "\n",
    "RMSE=rmse(y_evaluation_result,y_evalutation)\n",
    "\n",
    "print(\"RMSE Value based on train data set:\")\n",
    "print(RMSE)\n",
    "\n",
    "\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on the test set\n",
    "Now let's load up our test inputs and predict the fare_amounts for them using our learned weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./input/test.csv')\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_travel_vector_features(test_df)\n",
    "test_X = get_input_matrix(test_df)\n",
    "# Predict fare_amount on the test set using our model (w) trained on the training set.\n",
    "test_y_predictions = np.matmul(test_X, w).round(decimals = 2)\n",
    "\n",
    "# Write the predictions to a CSV file which we can submit to the competition.\n",
    "submission = pd.DataFrame(\n",
    "    {'key': test_df.key, 'fare_amount': test_y_predictions},\n",
    "    columns = ['key', 'fare_amount'])\n",
    "submission.to_csv('submission.csv', index = False)\n",
    "\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  RMSE: $5.74, if evaluated on the website to the website "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Improvement\n",
    "The output here will score an RMSE of $5.74, but you can do better than that! Here are some suggestions:\n",
    "\n",
    "Use more columns from the input data. Here we're only using the start/end GPS points from columns [pickup|dropoff]_[latitude|longitude]. Try to see if the other columns -- pickup_datetime and passenger_count -- can help improve your results.\n",
    "Use absolute location data rather than relative. Here we're only looking at the difference between the start and end points, but maybe the actual values -- indicating where in NYC the taxi is traveling -- would be useful.\n",
    "Use a non-linear model to capture more intricacies within the data.\n",
    "Try to find more outliers to prune, or construct useful feature crosses.\n",
    "Use the entire dataset -- here we're only using about 20% of the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
